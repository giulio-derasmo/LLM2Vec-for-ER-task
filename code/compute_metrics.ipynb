{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28044fb-b06c-4332-97c6-554109b598b5",
   "metadata": {},
   "source": [
    "### Load libr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1cfcc177-587d-4694-9715-caf7ba674ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe4cdf-866f-4404-93ca-c11058a8d95d",
   "metadata": {},
   "source": [
    "### Util funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "23f86c6e-1b45-4fef-8bd4-1a8439141996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(dataset_path):\n",
    "    '''Function for load the dataset'''\n",
    "    tableA = pd.read_csv(os.path.join(dataset_path, 'tableA.csv'))\n",
    "    tableB = pd.read_csv(os.path.join(dataset_path, 'tableB.csv'))\n",
    "    test = pd.read_csv(os.path.join(dataset_path, 'test.csv'))\n",
    "\n",
    "    return tableA, tableB, test\n",
    "\n",
    "def retrieve_testing_sample_only(dataset_path):\n",
    "    '''Function for retrieve only the rows in the tables that will be tested'''\n",
    "    tableA, tableB, test = load_all_data(dataset_path)\n",
    "\n",
    "    ## retrieve the testing rows\n",
    "    tableA_testingId = test.ltable_id.to_list()\n",
    "    tableB_testingId = test.rtable_id.to_list()\n",
    "    ## subsetting the original dataframe\n",
    "    testing_tableA = tableA[tableA.id.isin(tableA_testingId)].reset_index(drop=True)\n",
    "    testing_tableB = tableB[tableB.id.isin(tableB_testingId)].reset_index(drop=True)\n",
    "\n",
    "    return testing_tableA, testing_tableB\n",
    "\n",
    "def prepare_dataset(table, with_labels = True):\n",
    "  '''This function take as input a table and add a columns with:\n",
    "      - if schema agnostic: all attribute concatenated\n",
    "      - otherwise: consider the values of one or two specific attributes per\n",
    "                   dataset\n",
    "  '''\n",
    "  new_table = table.copy()\n",
    "  if not with_labels:\n",
    "    # we drop first attribute since is row_id\n",
    "    new_table.loc[:, 'attribute'] = new_table.apply(lambda sample: ' '.join(str(x) for x in sample.dropna()[1:]), axis = 1)\n",
    "  else:\n",
    "    new_table.loc[:, 'attribute'] = new_table.apply(lambda sample: ' '.join(f'<{k}> {v} </{k}>' for k, v in sample.dropna()[1:].to_dict().items()),  axis = 1)\n",
    "  return new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfa84f-1dbe-465b-9114-ebbb31de0db3",
   "metadata": {},
   "source": [
    "### Import and perform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "88be82ff-4bb3-4a3f-ad9b-1bc81af073fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths \n",
    "## csv files\n",
    "dataset_table_dir_path = \"data/datasets\"\n",
    "datasets_path = [os.path.join(dataset_name, 'exp_data') for dataset_name in os.listdir(dataset_table_dir_path)]\n",
    "datasets_name = [x.split('\\\\')[0] for x in datasets_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1ee34d5e-b604-4fe3-bdb1-87cc2cbde7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abt_buy_exp_data',\n",
       " 'dirty_dblp_acm_exp_data',\n",
       " 'dirty_dblp_scholar_exp_data',\n",
       " 'dirty_itunes_amazon_exp_data',\n",
       " 'dirty_walmart_amazon_exp_data']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "80173b3d-a244-4c9c-86f2-bfae26c55f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset_path in zip(datasets_name, datasets_path):\n",
    "    ## retrieve tables\n",
    "    testing_tableA, testing_tableB = retrieve_testing_sample_only(os.path.join(dataset_table_dir_path, dataset_path))\n",
    "    for type_of_model in ['simce', 'supervised']:\n",
    "        embedding_dir_path = f'embeddings2/{type_of_model}_embeddings'\n",
    "        for labels in [0,1]:\n",
    "            ## retrieve embeddings\n",
    "            embedding_dir_path_final = os.path.join(embedding_dir_path, dataset_name, 'with_separator') if labels else os.path.join(embedding_dir_path, dataset_name)\n",
    "            files = glob.glob(f'{embedding_dir_path_final}/*.pt')\n",
    "            record_embeddings_A = torch.load(files[0])\n",
    "            record_embeddings_B = torch.load(files[1])\n",
    "            ## add to dataframe\n",
    "            testing_tableA[f'{type_of_model}_{labels}'] = record_embeddings_A.tolist()\n",
    "            testing_tableB[f'{type_of_model}_{labels}'] = record_embeddings_B.tolist()\n",
    "    ## saves\n",
    "    testing_tableA.to_pickle(f'data/datasets_with_embeddings/{dataset_name}_tableA.pickle')\n",
    "    testing_tableB.to_pickle(f'data/datasets_with_embeddings/{dataset_name}_tableB.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d699bc-fd48-4c28-85c3-1d38c9e62274",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset_path in zip(datasets_name, datasets_path):\n",
    "    ## retrieve tables\n",
    "    testing_tableA, testing_tableB = retrieve_testing_sample_only(os.path.join(dataset_table_dir_path, dataset_path))\n",
    "    embedding_dir_path = f'embeddings2'\n",
    "    for labels in [0,1]:\n",
    "        ## retrieve embeddings\n",
    "        embedding_dir_path_final = os.path.join(embedding_dir_path, dataset_name, 'with_separator') if labels else os.path.join(embedding_dir_path, dataset_name)\n",
    "        files = glob.glob(f'{embedding_dir_path_final}/*.pt')\n",
    "        record_embeddings_A = torch.load(files[0])\n",
    "        record_embeddings_B = torch.load(files[1])\n",
    "        ## add to dataframe\n",
    "        testing_tableA[f'supervised_{labels}'] = record_embeddings_A.tolist()\n",
    "        testing_tableB[f'supervised_{labels}'] = record_embeddings_B.tolist()\n",
    "    ## saves\n",
    "    testing_tableA.to_pickle(f'data/datasets_with_embeddings2/{dataset_name}_tableA.pickle')\n",
    "    testing_tableB.to_pickle(f'data/datasets_with_embeddings2/{dataset_name}_tableB.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f163b-777e-4fa2-9c98-1aefef3bf3f1",
   "metadata": {},
   "source": [
    "#### 1. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0e86d33a-be10-4985-a2e9-95edee023a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e477eeb7-869f-4812-97c7-0bdc70f86e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for dataset_name, dataset_path in zip(datasets_name, datasets_path):\n",
    "    results[dataset_name] = {}\n",
    "    '''load the dataset'''\n",
    "    tableA = pd.read_pickle(f'data/datasets_with_embeddings2/{dataset_name}_tableA.pickle')\n",
    "    tableB = pd.read_pickle(f'data/datasets_with_embeddings2/{dataset_name}_tableB.pickle')\n",
    "    test = pd.read_csv(os.path.join('data/datasets', dataset_path, 'test.csv'))\n",
    "\n",
    "    ## merge\n",
    "    #modalities = ['simce_0','simce_1','supervised_0','supervised_1']\n",
    "    modalities = ['supervised_0','supervised_1']\n",
    "    for modality in modalities:\n",
    "        output = test.copy()\n",
    "        output = pd.merge(tableA[['id',modality]].rename(columns = {'id': 'ltable_id'}), output, on = 'ltable_id')\n",
    "        output = pd.merge(tableB[['id',modality]].rename(columns = {'id': 'rtable_id'}), output, on = 'rtable_id')\n",
    "        \n",
    "        preds = output[[f'{modality}_x',f'{modality}_y']].apply(lambda pair: int(cosine_similarity(pair[f'{modality}_x'], pair[f'{modality}_y'])>0.5), axis = 1)\n",
    "        tn, fp, fn, tp = confusion_matrix(output.label, preds, normalize = 'all').ravel()\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "        results[dataset_name][modality] = {\n",
    "            'precision': precision, \n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e56f60d1-7a41-496f-86ea-921038f61f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "reform = {(outerKey, innerKey): values for outerKey, innerDict in results.items() for innerKey, values in innerDict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "580e486d-1b97-4c00-ae37-a83223f71c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrrr}\\n\\\\toprule\\n & abt_buy_exp_data & dirty_dblp_acm_exp_data & dirty_dblp_scholar_exp_data & dirty_itunes_amazon_exp_data & dirty_walmart_amazon_exp_data \\\\\\\\\\n & supervised_0 & supervised_0 & supervised_0 & supervised_0 & supervised_0 \\\\\\\\\\n\\\\midrule\\nprecision & 0.107516 & 0.179612 & 0.186671 & 0.247706 & 0.094192 \\\\\\\\\\nrecall & 1.000000 & 1.000000 & 1.000000 & 1.000000 & 1.000000 \\\\\\\\\\nf1 & 0.194156 & 0.304527 & 0.314613 & 0.397059 & 0.172168 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_keep = [('abt_buy_exp_data', 'supervised_0'), ('dirty_dblp_acm_exp_data', 'supervised_0'), \n",
    "               ('dirty_dblp_scholar_exp_data', 'supervised_0'), ('dirty_itunes_amazon_exp_data','supervised_0'), \n",
    "               ('dirty_walmart_amazon_exp_data', 'supervised_0')]\n",
    "\n",
    "pd.DataFrame({k: reform[k] for k in key_to_keep}).to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1f8b5128-7318-4293-ad66-6726a66afce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">abt_buy_exp_data</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dirty_dblp_acm_exp_data</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dirty_dblp_scholar_exp_data</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dirty_itunes_amazon_exp_data</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dirty_walmart_amazon_exp_data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>supervised_0</th>\n",
       "      <th>supervised_1</th>\n",
       "      <th>supervised_0</th>\n",
       "      <th>supervised_1</th>\n",
       "      <th>supervised_0</th>\n",
       "      <th>supervised_1</th>\n",
       "      <th>supervised_0</th>\n",
       "      <th>supervised_1</th>\n",
       "      <th>supervised_0</th>\n",
       "      <th>supervised_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.107516</td>\n",
       "      <td>0.107516</td>\n",
       "      <td>0.179539</td>\n",
       "      <td>0.179539</td>\n",
       "      <td>0.186574</td>\n",
       "      <td>0.186346</td>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.094192</td>\n",
       "      <td>0.094192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.194156</td>\n",
       "      <td>0.194156</td>\n",
       "      <td>0.304422</td>\n",
       "      <td>0.304422</td>\n",
       "      <td>0.314475</td>\n",
       "      <td>0.314151</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.172168</td>\n",
       "      <td>0.172168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          abt_buy_exp_data              dirty_dblp_acm_exp_data               \\\n",
       "              supervised_0 supervised_1            supervised_0 supervised_1   \n",
       "precision         0.107516     0.107516                0.179539     0.179539   \n",
       "recall            1.000000     1.000000                1.000000     1.000000   \n",
       "f1                0.194156     0.194156                0.304422     0.304422   \n",
       "\n",
       "          dirty_dblp_scholar_exp_data               \\\n",
       "                         supervised_0 supervised_1   \n",
       "precision                    0.186574     0.186346   \n",
       "recall                       1.000000     1.000000   \n",
       "f1                           0.314475     0.314151   \n",
       "\n",
       "          dirty_itunes_amazon_exp_data               \\\n",
       "                          supervised_0 supervised_1   \n",
       "precision                     0.247706     0.247706   \n",
       "recall                        1.000000     1.000000   \n",
       "f1                            0.397059     0.397059   \n",
       "\n",
       "          dirty_walmart_amazon_exp_data               \n",
       "                           supervised_0 supervised_1  \n",
       "precision                      0.094192     0.094192  \n",
       "recall                         1.000000     1.000000  \n",
       "f1                             0.172168     0.172168  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68d4e9-5519-46c0-9c77-a900ca5fac90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
